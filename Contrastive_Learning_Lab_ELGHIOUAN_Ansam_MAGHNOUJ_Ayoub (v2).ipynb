{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d56f5655-446f-4c5b-a3df-ea159fd0fd3e",
      "metadata": {
        "id": "d56f5655-446f-4c5b-a3df-ea159fd0fd3e"
      },
      "source": [
        "# Contrastive Learning Lab Binôme_8\n",
        "### Objectif du Lab\n",
        "Ce notebook explore le Contrastive Learning, une technique d'apprentissage auto-supervisé qui vise à apprendre des représentations de données en rapprochant les exemples similaires et en éloignant les exemples différents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6df0478-8b0c-467b-a9ed-16bd7501c576",
      "metadata": {
        "id": "a6df0478-8b0c-467b-a9ed-16bd7501c576"
      },
      "outputs": [],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae2f6d4-3f7b-4ebe-8575-573346c09ca8",
      "metadata": {
        "id": "bae2f6d4-3f7b-4ebe-8575-573346c09ca8",
        "outputId": "e1c5a284-0fb0-4310-eac8-a19d62562ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Données CIFAR-10 chargées avec succès !\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Définition des transformations pour prétraiter les images\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),  # Recadrage aléatoire\n",
        "    transforms.RandomHorizontalFlip(),  # Flip horizontal aléatoire\n",
        "    transforms.ToTensor(),  # Conversion en tenseur PyTorch\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalisation des pixels\n",
        "])\n",
        "\n",
        "# Chargement du dataset CIFAR-10\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Création des DataLoaders pour optimiser l'entraînement\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"Données CIFAR-10 chargées avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941c9211-ea3f-4a4d-9739-6408bc66dd7d",
      "metadata": {
        "id": "941c9211-ea3f-4a4d-9739-6408bc66dd7d",
        "outputId": "4a864a17-ff4f-4248-d197-a79728f01865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle Contrastif défini avec succès !\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Définition du modèle Contrastive Learning avec des noms en français\n",
        "class ModeleContrastif(nn.Module):\n",
        "    def __init__(self, dimension_embedding=128):\n",
        "        super(ModeleContrastif, self).__init__()\n",
        "        self.extracteur_caracteristiques = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8192, dimension_embedding),\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        return F.normalize(self.extracteur_caracteristiques(images), p=2, dim=1)  # Normalisation L2\n",
        "\n",
        "# Instanciation du modèle\n",
        "modele_contrastif = ModeleContrastif()\n",
        "print(\"Modèle Contrastif défini avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d0b717-b931-47b6-8739-4a7065b7bac3",
      "metadata": {
        "id": "35d0b717-b931-47b6-8739-4a7065b7bac3",
        "outputId": "538182d3-ecf5-42b7-e33d-245650bd698b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fonction de perte contrastive définie avec succès !\n"
          ]
        }
      ],
      "source": [
        "class PerteContrastive(nn.Module):\n",
        "    def __init__(self, marge=1.0):\n",
        "        super(PerteContrastive, self).__init__()\n",
        "        self.marge = marge\n",
        "\n",
        "    def forward(self, embedding1, embedding2, label):\n",
        "        distance = F.pairwise_distance(embedding1, embedding2)\n",
        "        perte = label * torch.pow(distance, 2) + (1 - label) * torch.pow(torch.clamp(self.marge - distance, min=0.0), 2)\n",
        "        return perte.mean()\n",
        "\n",
        "# Instanciation de la fonction de perte\n",
        "fonction_perte = PerteContrastive()\n",
        "print(\"Fonction de perte contrastive définie avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc59584-f00d-4db6-9f85-8d4df6dfaf7e",
      "metadata": {
        "scrolled": true,
        "id": "7fc59584-f00d-4db6-9f85-8d4df6dfaf7e",
        "outputId": "713dffd6-3888-4514-f772-215ee4f7e0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle rechargé depuis le dernier checkpoint !\n",
            "Début de l'entraînement - Époque 1/10\n",
            " Lot 1/391 - Perte courante : 0.0905\n",
            " Lot 2/391 - Perte courante : 0.1490\n",
            " Lot 3/391 - Perte courante : 0.0900\n",
            " Lot 4/391 - Perte courante : 0.0778\n",
            " Lot 5/391 - Perte courante : 0.1328\n",
            " Lot 6/391 - Perte courante : 0.1239\n",
            " Lot 7/391 - Perte courante : 0.0971\n",
            " Lot 8/391 - Perte courante : 0.1311\n",
            " Lot 9/391 - Perte courante : 0.1083\n",
            " Lot 10/391 - Perte courante : 0.0909\n",
            "Entraînement interrompu manuellement. Sauvegarde du modèle en cours...\n",
            "Modèle sauvegardé avant l'interruption. Relance possible depuis le checkpoint !\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Définition de l'optimiseur\n",
        "optimiseur = optim.Adam(modele_contrastif.parameters(), lr=0.001)\n",
        "\n",
        "# Boucle d'entraînement avec arrêt automatique + gestion des interruptions\n",
        "def entrainer_modele(epochs=10, seuil_perte=0.01, checkpoint_interval=5):\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            perte_totale = 0.0\n",
        "\n",
        "            print(f\"Début de l'entraînement - Époque {epoch+1}/{epochs}\")\n",
        "\n",
        "            for i, (images, labels) in enumerate(train_loader):\n",
        "                images = images.to(torch.device(\"cpu\"))\n",
        "\n",
        "                if images.size(0) < 2:\n",
        "                    print(f\" Lot {i}: Pas assez d'images pour créer des paires, on ignore cette itération.\")\n",
        "                    continue\n",
        "\n",
        "                # Création des paires d'images\n",
        "                image1, image2 = images[:64], images[64:]\n",
        "                labels_paires = (labels[:64] == labels[64:]).float()\n",
        "\n",
        "                # Passage des images à travers le modèle\n",
        "                embedding1 = modele_contrastif(image1)\n",
        "                embedding2 = modele_contrastif(image2)\n",
        "\n",
        "                # Calcul de la perte contrastive\n",
        "                perte = fonction_perte(embedding1, embedding2, labels_paires)\n",
        "\n",
        "                # Optimisation\n",
        "                optimiseur.zero_grad()\n",
        "                perte.backward()\n",
        "                optimiseur.step()\n",
        "\n",
        "                perte_totale += perte.item()\n",
        "\n",
        "                # Affichage après chaque lot pour suivre la progression\n",
        "                print(f\" Lot {i+1}/{len(train_loader)} - Perte courante : {perte.item():.4f}\")\n",
        "\n",
        "            # Vérification pour arrêt automatique\n",
        "            print(f\" Époque {epoch+1}/{epochs} terminée - Perte totale : {perte_totale:.4f}\", flush=True)\n",
        "\n",
        "            if perte_totale < seuil_perte:\n",
        "                print(f\" Arrêt automatique activé : perte ({perte_totale:.4f}) inférieure au seuil ({seuil_perte}) !\")\n",
        "                break\n",
        "\n",
        "            # Sauvegarde automatique du modèle tous les {checkpoint_interval} epochs\n",
        "            if (epoch + 1) % checkpoint_interval == 0:\n",
        "                torch.save(modele_contrastif.state_dict(), f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "                print(f\"Checkpoint sauvegardé à l'époque {epoch+1} !\")\n",
        "\n",
        "        print(\" Entraînement terminé avec succès !\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Entraînement interrompu manuellement. Sauvegarde du modèle en cours...\")\n",
        "        torch.save(modele_contrastif.state_dict(), \"checkpoint_interruption.pth\")\n",
        "        print(\"Modèle sauvegardé avant l'interruption. Relance possible depuis le checkpoint !\")\n",
        "\n",
        "\n",
        "# Charger le modèle depuis le dernier checkpoint avant de reprendre l'entraînement\n",
        "try:\n",
        "    modele_contrastif.load_state_dict(torch.load(\"checkpoint_interruption.pth\"))\n",
        "    print(\"Modèle rechargé depuis le dernier checkpoint !\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Aucun checkpoint précédent trouvé. Début d'un entraînement complet.\")\n",
        "\n",
        "#  Relancer l'entraînement à partir du checkpoint ou depuis zéro\n",
        "entrainer_modele(epochs=10, seuil_perte=0.01, checkpoint_interval=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac74cdf-f7c3-4596-be17-30fd4a119013",
      "metadata": {
        "scrolled": true,
        "id": "fac74cdf-f7c3-4596-be17-30fd4a119013",
        "outputId": "bd4b7c92-d387-47fb-ffec-399b616dff5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Début de l'évaluation du modèle...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Évaluation en cours: 100%|████████████████████| 79/79 [00:40<00:00,  1.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Précision du modèle : 89.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Importer tqdm (pour afficher la barre de progression)\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluer_modele():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    print(\"Début de l'évaluation du modèle...\")\n",
        "\n",
        "    with torch.no_grad():  # Désactivation du calcul du gradient\n",
        "        for images, labels in tqdm(test_loader, desc=\"Évaluation en cours\"):\n",
        "            images = images.to(torch.device(\"cpu\"))\n",
        "\n",
        "            # Calculer la taille du batch actuel\n",
        "            batch_size = images.size(0)\n",
        "            moitié_batch = batch_size // 2  # Diviser dynamiquement\n",
        "\n",
        "            if moitié_batch == 0:\n",
        "                continue\n",
        "\n",
        "            # Création des paires de taille adaptative\n",
        "            image1, image2 = images[:moitié_batch], images[moitié_batch:]\n",
        "            labels_paires = (labels[:moitié_batch] == labels[moitié_batch:]).float()\n",
        "\n",
        "            # Passage des images dans le modèle\n",
        "            embedding1 = modele_contrastif(image1)\n",
        "            embedding2 = modele_contrastif(image2)\n",
        "\n",
        "            # Calcul de la distance entre les embeddings\n",
        "            distance = torch.norm(embedding1 - embedding2, p=2, dim=1)\n",
        "\n",
        "            # Prédiction : si distance faible → images similaires\n",
        "            predictions = (distance < 0.5).float()\n",
        "\n",
        "            # Calcul du nombre de prédictions correctes\n",
        "            total_correct += (predictions == labels_paires).sum().item()\n",
        "            total_samples += labels_paires.size(0)\n",
        "\n",
        "        # Calcul et affichage de la précision\n",
        "        if total_samples > 0:\n",
        "            precision = total_correct / total_samples * 100\n",
        "            print(f\"Précision du modèle : {precision:.2f}%\", flush=True)\n",
        "        else:\n",
        "            print(\"Impossible de calculer la précision (pas assez d'échantillons).\")\n",
        "\n",
        "# Lancer l'évaluation\n",
        "evaluer_modele()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}